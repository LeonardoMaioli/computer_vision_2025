<!DOCTYPE html>
<html lang="pt-br">
<head>
  <meta charset="UTF-8">
  <title>Trabalho</title>
  <link rel="stylesheet" href="style_trabalho.css">
</head>
<body>

<header style="display: flex; align-items: center; justify-content: left; gap: 15px; padding: 10px;">
    <a href="index.html">
        <img src="home/libras.png" alt="Voltar √† P√°gina Inicial" style="height: 40px; cursor: pointer; padding-right: 150px; filter: invert(1);">
    </a>
    <span>Trabalho de Vis√£o Computacional - Sistema tradutor de Libras para texto em portugu√™s</span>
</header>

<div class="container">
    <div class="sidebar">
        <button onclick="showContent('home')">Equipe</button>
        <button onclick="showContent('cenario-aplicacao')">Etapa 1 - CA</button>
        <button onclick="showContent('modelagem-funcional')">Etapa 2 - MF</button>
        <button onclick="showContent('seminario-pt1')">Etapa 3 - Sem 1</button>
        <button onclick="showContent('spv')">Etapa 4 - SPV</button>
        <button onclick="showContent('laboratorio-experimental')">Etapa 5 - Lex</button>
        <button onclick="showContent('teste-campo')">Etapa 6 - TC</button>
        <button onclick="showContent('relatorio-final')">Etapa 7 - RFT</button>
        <button onclick="showContent('seminario-pt2')">Etapa 8 - Sem 2</button>
        <button onclick="showContent('referencias')">Refer√™ncias</button>
    </div>

    <div class="content">
        <div id="home" class="content-section">
            <h2>Integrantes</h2>
            <ul style="list-style-type: disc; padding-left: 20px;">
                <li>Leonardo Severgnine Maioli ‚Äì 11201920579</li>
                <li>Ricardo Javurek Rihan ‚Äì 11201920897</li>
                <li>Tiago Luiz Silva de Ara√∫jo Pereira ‚Äì 11013316</li>
            </ul>

            <h2 style="margin-top: 30px;">Tema do Trabalho</h2>
            <p><strong>Sistema tradutor de Libras para texto utilizando t√©cnicas de Vis√£o Computacional - Aplica√ß√£o em contextos cl√≠nicos e hospitalares. </strong></p>
            <p>O projeto visa o desenvolvimento de um prot√≥tipo baseado em t√©cnicas de Vis√£o Computacional para traduzir sinais de Libras
               em tempo real para texto em portugu√™s. O sistema ter√° como foco a utiliza√ß√£o em cl√≠nicas e hospitais, sendo capaz de reconhecer 
               sinais usados em triagem de pronto-socorro ou em consultas cl√≠nicas b√°sicas como sintomas, dores, respostas simples como 
               "sim", "n√£o", "febre", "dor de cabe√ßa", entre outras. Dessa maneira, o sistema ser√° especialmente √∫til em contextos hospitalares
               e cl√≠nicos, permitindo que pacientes surdos consigam se comunicar com profissionais da sa√∫de de forma mais eficiente. 
               A solu√ß√£o capturar√° v√≠deos de sinais, processar√° os frames, identificar√° os gestos e os traduzir√° em palavras e 
               frases compreens√≠veis.</p>

            <div style="text-align: center; margin-top: 30px;">
                <img src="home/img_home.png" alt="Imagem ilustrativa do sistema" style="max-width: 80%; height: auto;">
            </div>
        </div>
        <div id="cenario-aplicacao" class="content-section hidden">
            <h2 style="display: flex; align-items: center; gap: 10px; margin-bottom: 5px;">
                Contexto e Cen√°rio de Aplica√ß√£o (CA)
                <a href="etapa1-CA/contexto-cenario-aplicacao.pdf" target="_blank" title="Abrir PDF">
                    üìÑ
                </a>
            </h2>

            <h3 style="margin-bottom: 5px; margin-top: 5px;">Contexto do Problema</h3>
            <p>A L√≠ngua Brasileira de Sinais (Libras) √© a principal forma de comunica√ß√£o de milhares de pessoas surdas no Brasil. Trata-se de uma l√≠ngua visual-motora com estrutura gramatical pr√≥pria, e n√£o √© universal, com cada pa√≠s possuindo sua pr√≥pria l√≠ngua de sinais. Apesar de ter sido reconhecida oficialmente no Brasil como meio legal de comunica√ß√£o em 2002 (Lei n¬∫ 10.436/2002), a falta de profissionais int√©rpretes de Libras e a baixa inclus√£o digital para a comunidade surda ainda s√£o barreiras significativas. Em muitos servi√ßos p√∫blicos, empresas e at√© mesmo em ambientes educacionais, a comunica√ß√£o entre surdos e ouvintes √© limitada pela aus√™ncia de tradutores humanos.</p>

            <h4 style="margin-bottom: 5px; margin-top: 5px;">Escolha do Tema e Justificativa</h4>
            <p>Durante a etapa de entrevistas emp√°ticas deste trabalho, os membros do grupo entrevistaram pessoas buscando entender com empatia os problemas que existiam em seus cotidianos, ou de pessoas conhecidas, em que a solu√ß√£o poderia envolver o uso de recursos como c√¢meras, imagens, v√≠deos, smartphones e as t√©cnicas de vis√£o computacional. Nesta etapa, diversos problemas nesse contexto foram levantados e podem ser melhor entendidos nos relatos de entrevistas entregues na etapa anterior.</p>

            <p>A partir dos problemas levantados pelos entrevistados, a equipe optou por trabalhar com o problema da dificuldade de comunica√ß√£o entre surdos e ouvintes, devido √† sua relev√¢ncia social e ao impacto direto na inclus√£o de pessoas surdas em diferentes contextos, como sa√∫de, educa√ß√£o e atendimento ao p√∫blico. Al√©m disso, uma an√°lise de mercado mostrou que existem alguns aplicativos voltados para a tradu√ß√£o de texto, voz ou v√≠deo para Libras, como o Hand Talk e o VLibras, que utilizam avatares virtuais para apresentar os sinais de Libras a partir de um conte√∫do em portugu√™s. No entanto, essas solu√ß√µes atuam apenas na dire√ß√£o Portugu√™s ‚Üí Libras, n√£o sendo capazes de realizar a tradu√ß√£o inversa, ou seja, de Libras (captada por c√¢mera) para texto em portugu√™s.</p>

            <p>Continuando com a pesquisa, a equipe encontrou artigos cient√≠ficos que tamb√©m apontam a exist√™ncia de alguns trabalhos voltados para a tradu√ß√£o de Libras para texto, como o realizado por Nath et al. (2017), que prop√¥s um modelo utilizando t√©cnicas de processamento de imagem, como o algoritmo de Casca Convexo (Convex Hull Algorithm) e o algoritmo de Correspond√™ncia de Modelos (Template Matching Algorithm), para realizar o reconhecimento de linguagem de sinais. Tamb√©m foi encontrado o trabalho de Papatsimouli et al. (2023), que realizou uma revis√£o sistem√°tica dos avan√ßos recentes em sistemas de tradu√ß√£o de linguagem de sinais em tempo real, com foco especial na integra√ß√£o dessas tecnologias com a Internet das Coisas (IoT).</p>

            <p>Apesar desses avan√ßos iniciais, muitos desses estudos est√£o em fase experimental, com vocabul√°rio limitado e foco em ambientes controlados de laborat√≥rio. Dessa forma, fica claro que ainda existe uma lacuna significativa na oferta de solu√ß√µes pr√°ticas e acess√≠veis para o p√∫blico geral que possibilitem a tradu√ß√£o autom√°tica de Libras para texto, especialmente em cen√°rios reais com variabilidade de ilumina√ß√£o, fundo, √¢ngulo de c√¢mera e velocidade dos sinais. Com base nisso, a equipe prop√µe o desenvolvimento de um sistema de tradu√ß√£o de Libras para texto com foco em contextos cl√≠nicos e hospitalares, visando atender a uma demanda identificada nas entrevistas e que possa trazer bons resultados futuros para essa √°rea.</p>

            <h3 style="margin-bottom: 5px; margin-top: 5px;">Sistema de Tradu√ß√£o</h3>
            <p>O objetivo principal do projeto √© desenvolver um prot√≥tipo funcional de um sistema de tradu√ß√£o autom√°tica de Libras para texto escrito em portugu√™s, utilizando t√©cnicas de Vis√£o Computacional e Processamento de V√≠deo. Al√©m disso, o sistema ter√° um foco espec√≠fico para utiliza√ß√£o em cl√≠nicas e hospitais, ou seja, ser√° capaz de reconhecer sinais espec√≠ficos usados em triagem de pronto-socorro ou em consultas cl√≠nicas b√°sicas como sintomas, dores, respostas simples como "sim", "n√£o", "febre", "dor de cabe√ßa", entre outras.</p>

            <p>Espera-se que o resultado final para o usu√°rio seja uma aplica√ß√£o capaz de capturar, por meio de uma webcam ou c√¢mera de celular, o v√≠deo de uma pessoa realizando sinais em Libras. O sistema ser√° respons√°vel por processar os frames desse v√≠deo, reconhecer os gestos feitos pelas m√£os e bra√ßos e exibir em uma tela a tradu√ß√£o correspondente em forma de texto.</p>

            <h4 style="margin-bottom: 5px; margin-top: 5px;">Utiliza√ß√£o do Sistema</h4>
            <p>O sistema ser√° projetado para possuir uma interface simples e amig√°vel aos usu√°rios, sejam eles pessoas surdas ou ouvintes. O processo de utiliza√ß√£o do sistema ocorrer√° nas seguintes etapas:</p>

            <ol style="padding-left: 20px;">
                <li>Para um in√≠cio de comunica√ß√£o entre um ouvinte e uma pessoa surda, a c√¢mera conectada a uma tela (celular ou notebook) dever√° ser posicionada de frente para essa √∫ltima, enquanto a tela fica com a outra pessoa.</li>
                <li>Ao ligar o sistema, ele come√ßar√° a capturar o v√≠deo em tempo real do surdo se comunicando em Libras.</li>
                <li>A aplica√ß√£o ir√° reconhecer os sinais de Libras feitos pela pessoa.</li>
                <li>Ap√≥s o processamento, o sistema exibir√° na tela o texto traduzido para o portugu√™s.</li>
                <li>O usu√°rio ouvinte poder√° ler esse texto e ent√£o entender a mensagem transmitida pela pessoa surda.</li>
            </ol>

            <h4 style="margin-bottom: 5px; margin-top: 5px;">Benef√≠cio do Sistema</h4>
            <p>O principal benef√≠cio do sistema de tradu√ß√£o ser√° a promo√ß√£o da inclus√£o comunicacional em contextos de atendimento em sa√∫de, reduzindo barreiras lingu√≠sticas entre profissionais da sa√∫de e pessoas surdas.</p>

            <p>Com a utiliza√ß√£o do sistema, m√©dicos, enfermeiros, atendentes de recep√ß√£o e outros profissionais poder√£o compreender de forma r√°pida e eficiente o que o paciente surdo est√° comunicando em Libras, mesmo sem possuir conhecimento pr√©vio da l√≠ngua de sinais. A aplica√ß√£o pr√°tica deste sistema ser√° especialmente √∫til em momentos de triagem, consultas cl√≠nicas b√°sicas e atendimentos de emerg√™ncia, nos quais a comunica√ß√£o precisa ser objetiva, r√°pida e clara.</p>

            <p>Al√©m de facilitar o atendimento e garantir maior seguran√ßa ao paciente surdo, o sistema tamb√©m poder√° servir como base para futuras pesquisas na √°rea de tradu√ß√£o autom√°tica de l√≠nguas de sinais, ampliando o vocabul√°rio reconhecido, melhorando a precis√£o dos algoritmos e contribuindo para o avan√ßo da acessibilidade digital no Brasil.</p>

        </div>
        <div id="modelagem-funcional" class="content-section hidden">
            <h2 style="display: flex; align-items: center; gap: 10px; margin-bottom: 5px;">
                Modelagem Funcional do Sistema (MF)
                <a href="etapa2-MF/modelagem-funcional.pdf" target="_blank" title="Abrir PDF">
                    üìÑ
                </a>
            </h2>

            <p>Este documento traz uma breve vis√£o geral sobre o cen√°rio de aplica√ß√£o escolhido pelo grupo para o sistema de tradu√ß√£o de Libras para texto. Na sequ√™ncia, √© apresentado um diagrama de blocos seguido da descri√ß√£o de cada parte do mesmo, com o objetivo de apresentar as etapas do funcionamento do sistema, seu fluxo de dados e o detalhamento das informa√ß√µes de entrada, de sa√≠da e do processamento realizado em cada bloco.</p>

            <h3 style="margin-bottom: 5px; margin-top: 5px;">Vis√£o Geral do Sistema</h3>
            <p>O sistema a ser desenvolvido pela equipe ser√° um prot√≥tipo que utilizar√° t√©cnicas de Vis√£o Computacional e Processamento de V√≠deo para traduzir sinais espec√≠ficos de Libras para texto escrito em portugu√™s. Al√©m disso, o sistema ter√° como foco a utiliza√ß√£o em cl√≠nicas e hospitais, sendo capaz de reconhecer sinais usados em triagem de pronto-socorro ou em consultas cl√≠nicas b√°sicas como sintomas, dores, respostas simples como "sim", "n√£o", "febre", "dor de cabe√ßa", entre outras.</p>

            <p>O sistema ser√° composto por alguns m√≥dulos que atuam de forma sequencial. A entrada principal ser√° um v√≠deo capturado em tempo real de uma pessoa sinalizando em Libras. O processamento envolver√° a captura desse v√≠deo, pr√©-processamento de frames, extra√ß√£o de caracter√≠sticas, classifica√ß√£o dos sinais e, por fim, a exibi√ß√£o do texto traduzido.</p>

            <h3 style="margin-bottom: 5px; margin-top: 5px;">Diagrama de Blocos</h3>
            <img src="etapa2-MF/diagrama_blocos_etapa2_trabalho.png" alt="Diagrama de Blocos" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">

            <h3 style="margin-bottom: 5px; margin-top: 5px;">Descri√ß√£o dos Blocos do Sistema</h3>
            <table border="1" cellpadding="10" cellspacing="0" style="border-collapse: collapse; width: 100%;">
                <thead>
                    <tr style="background-color: #f0f0f0;">
                        <th>Bloco</th>
                        <th>Entrada</th>
                        <th>Processamento</th>
                        <th>Sa√≠da</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Captura de v√≠deo com c√¢mera</td>
                        <td>V√≠deo em tempo real</td>
                        <td>Captura cont√≠nua de frames</td>
                        <td>Sequ√™ncia de frames</td>
                    </tr>
                    <tr>
                        <td>Pr√©-processamento dos frames</td>
                        <td>Frames</td>
                        <td>Remo√ß√£o de ru√≠do, filtros, ajuste de contraste</td>
                        <td>Imagens tratadas</td>
                    </tr>
                    <tr>
                        <td>Detec√ß√£o de regi√µes de interesse</td>
                        <td>Imagem tratada</td>
                        <td>Segmenta√ß√£o para isolar m√£os e bra√ßos</td>
                        <td>Regi√µes de interesse</td>
                    </tr>
                    <tr>
                        <td>Extra√ß√£o de caracter√≠sticas</td>
                        <td>Regi√£o de interesse</td>
                        <td>Extra√ß√£o de formas, contornos, pontos</td>
                        <td>Vetor de caracter√≠sticas</td>
                    </tr>
                    <tr>
                        <td>Classifica√ß√£o de sinais</td>
                        <td>Vetor de caracter√≠sticas</td>
                        <td>Modelo de Machine Learning para reconhecer sinais</td>
                        <td>R√≥tulo do sinal</td>
                    </tr>
                    <tr>
                        <td>Gera√ß√£o de texto</td>
                        <td>R√≥tulo do sinal</td>
                        <td>Convers√£o do r√≥tulo reconhecido para texto</td>
                        <td>Texto em portugu√™s</td>
                    </tr>
                    <tr>
                        <td>Exibi√ß√£o do texto</td>
                        <td>Texto</td>
                        <td>Exibi√ß√£o na interface</td>
                        <td>Texto exibido na tela</td>
                    </tr>
                </tbody>
            </table>

            <h3 style="margin-bottom: 5px; margin-top: 5px;">Tecnologias Previstas</h3>
            <ul style="padding-left: 20px;">
                <li><strong>Linguagem de programa√ß√£o:</strong> Python</li>
                <li><strong>Bibliotecas:</strong> OpenCV, TensorFlow/Keras, etc.</li>
                <li><strong>Dispositivo de Entrada:</strong> Webcam ou c√¢mera de celular</li>
                <li><strong>Dispositivo de Sa√≠da:</strong> Tela de computador ou smartphone</li>
            </ul>
        </div>
        <div id="seminario-pt1" class="content-section hidden">
            <h2>Semin√°rio 1</h2>
            <div id="slides-container" style="position: relative; max-width: 800px; margin: auto;">

                <!-- Slide 1 -->
                <div class="slide" style="display: block;">
                <h3>Problema</h3>
                <p>A L√≠ngua Brasileira de Sinais (Libras) √© a principal forma de comunica√ß√£o da comunidade surda no Brasil. Apesar de reconhecida por lei, a falta de profissionais int√©rpretes de Libras e a baixa inclus√£o digital para a comunidade surda ainda s√£o barreiras significativas na comunica√ß√£o de pessoas surdas com ouvintes em diversas √°reas.
                </div>
                
                <!-- Slide 2 -->
                <div class="slide" style="display: block;">
                <h3>Contexto do Problema</h3>
                <p>Ao buscarmos exemplos de onde esse problema aparece, notamos que ele est√° em toda parte, em servi√ßos p√∫blicos essenciais como os de sa√∫de, em empresas e em ambientes educacionais, a comunica√ß√£o entre surdos e ouvintes √© limitada.</p>
                </div>

                <!-- Slide 3 -->
                <div class="slide" style="display: none;">
                <h3>Justificativa do Projeto e Motiva√ß√£o</h3>
                <p>Durante a etapa de entrevistas emp√°ticas, esse problema da dificuldade de comunica√ß√£o da comunidade surda em diversas √°reas foi identificado em uma dessas entrevistas. A equipe optou por trabalhar com esse problema devido √† sua relev√¢ncia social e ao impacto direto na inclus√£o de pessoas surdas em diferentes contextos, como sa√∫de, educa√ß√£o e atendimento ao p√∫blico. </p>
                <p>Al√©m disso, esse levantamento levou a equipe a pesquisar mais sobre o tema e entender como o problema vem sendo tratado atualmente com algumas solu√ß√µes existentes.</p>
                <p>Uma an√°lise de mercado mostrou que existem alguns aplicativos voltados para a tradu√ß√£o de texto ou voz para Libras, como o Hand Talk e o VLibras, que utilizam avatares virtuais para apresentar os sinais de Libras a partir de um conte√∫do textual em portugu√™s. No entanto, essas solu√ß√µes atuam apenas na dire√ß√£o Portugu√™s ‚Üí Libras, n√£o sendo capazes de realizar a tradu√ß√£o inversa.</p>
                <p>Artigos cient√≠ficos encontrados pela equipe tamb√©m apontam a exist√™ncia de alguns trabalhos voltados para a tradu√ß√£o de Libras para texto. No entanto, muitos desses estudos est√£o em fase experimental, com vocabul√°rio limitado e foco em ambientes controlados de laborat√≥rio. Dessa forma, ficou claro que ainda existe uma lacuna significativa na oferta de solu√ß√µes pr√°ticas e acess√≠veis em diferentes √°reas.</p>
                <p>Com base nisso, a equipe prop√µe o desenvolvimento de um sistema de tradu√ß√£o de Libras para texto com foco em contextos cl√≠nicos e hospitalares, visando atender a uma demanda identificada nas entrevistas e que possa trazer bons resultados futuros para essa √°rea.</p>
                </div>

                <!-- Slide 4 -->
                <div class="slide" style="display: none;">
                <h3>Objetivo do Sistema e Benef√≠cios</h3>
                <p>O objetivo principal do projeto √© desenvolver um prot√≥tipo funcional de um sistema de tradu√ß√£o autom√°tica de Libras para texto escrito em portugu√™s, utilizando t√©cnicas de Vis√£o Computacional e Processamento de V√≠deo. Al√©m disso, o sistema ter√° um foco espec√≠fico para utiliza√ß√£o em cl√≠nicas e hospitais, ou seja, ser√° capaz de reconhecer sinais espec√≠ficos usados em triagem de pronto-socorro ou em consultas cl√≠nicas b√°sicas como sintomas, dores, respostas simples como "sim", "n√£o", "febre", "dor de cabe√ßa", entre outras.</p>
                <p>O principal benef√≠cio do sistema de tradu√ß√£o ser√° a promo√ß√£o da inclus√£o comunicacional em contextos de atendimento em sa√∫de, reduzindo barreiras lingu√≠sticas entre profissionais da sa√∫de e pessoas surdas.</p>
                <img src="home/img_home.png" alt="Exemplifica√ß√£o do Sistema" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
                </div>

                <!-- Slide 5 -->
                <div class="slide" style="display: none;">
                <h3>Funcionamento do Sistema</h3>
                <p>O sistema ser√° composto por alguns m√≥dulos que ir√£o atuar de forma sequencial. A entrada principal ser√° um v√≠deo capturado em tempo real de uma pessoa sinalizando em Libras. O processamento envolver√° a captura desse v√≠deo, pr√©-processamento de frames, extra√ß√£o de caracter√≠sticas, classifica√ß√£o dos sinais e, por fim, a exibi√ß√£o do texto traduzido.</p>
                </div>

                <!-- Slide 6 -->
                <div class="slide" style="display: none;">
                    <h3>Arquitetura Funcional do Sistema - Diagrama de Blocos</h3>
                    <img src="etapa2-MF/diagrama_blocos_etapa2_trabalho.png" alt="Diagrama de Blocos" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">

                    <h3 style="margin-bottom: 5px; margin-top: 5px;">Descri√ß√£o dos Blocos do Sistema</h3>
                    <table border="1" cellpadding="10" cellspacing="0" style="border-collapse: collapse; width: 100%;">
                        <thead>
                            <tr style="background-color: #f0f0f0;">
                                <th>Bloco</th>
                                <th>Entrada</th>
                                <th>Processamento</th>
                                <th>Sa√≠da</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Captura de v√≠deo com c√¢mera</td>
                                <td>V√≠deo em tempo real</td>
                                <td>Captura cont√≠nua de frames</td>
                                <td>Sequ√™ncia de frames</td>
                            </tr>
                            <tr>
                                <td>Pr√©-processamento dos frames</td>
                                <td>Frames</td>
                                <td>Remo√ß√£o de ru√≠do, filtros, ajuste de contraste</td>
                                <td>Imagens tratadas</td>
                            </tr>
                            <tr>
                                <td>Detec√ß√£o de regi√µes de interesse</td>
                                <td>Imagem tratada</td>
                                <td>Segmenta√ß√£o para isolar m√£os e bra√ßos</td>
                                <td>Regi√µes de interesse</td>
                            </tr>
                            <tr>
                                <td>Extra√ß√£o de caracter√≠sticas</td>
                                <td>Regi√£o de interesse</td>
                                <td>Extra√ß√£o de formas, contornos, pontos</td>
                                <td>Vetor de caracter√≠sticas</td>
                            </tr>
                            <tr>
                                <td>Classifica√ß√£o de sinais</td>
                                <td>Vetor de caracter√≠sticas</td>
                                <td>Modelo de Machine Learning para reconhecer sinais</td>
                                <td>R√≥tulo do sinal</td>
                            </tr>
                            <tr>
                                <td>Gera√ß√£o de texto</td>
                                <td>R√≥tulo do sinal</td>
                                <td>Convers√£o do r√≥tulo reconhecido para texto</td>
                                <td>Texto em portugu√™s</td>
                            </tr>
                            <tr>
                                <td>Exibi√ß√£o do texto</td>
                                <td>Texto</td>
                                <td>Exibi√ß√£o na interface</td>
                                <td>Texto exibido na tela</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <!-- Slide 7 -->
                <div class="slide" style="display: none;">
                    <h3 style="margin-bottom: 5px; margin-top: 5px;">Tecnologias Previstas</h3>
                    <ul style="padding-left: 20px;">
                        <li><strong>Linguagem de programa√ß√£o:</strong> Python</li>
                        <li><strong>Bibliotecas:</strong> OpenCV, TensorFlow/Keras, etc.</li>
                        <li><strong>Dispositivo de Entrada:</strong> Webcam ou c√¢mera de celular</li>
                        <li><strong>Dispositivo de Sa√≠da:</strong> Tela de computador ou smartphone</li>
                    </ul>
                </div>

                <!-- Navega√ß√£o -->
                <div style="text-align: center; margin-top: 20px;">
                <button onclick="changeSlide(-1)">‚óÄ Anterior</button>
                <button onclick="changeSlide(1)">Pr√≥ximo ‚ñ∂</button>
                </div>
            </div>
        </div>

            <script>
            let currentSlide = 0;
            const slides = document.querySelectorAll("#slides-container .slide");

            function showSlide(index) {
                slides.forEach((slide, i) => {
                slide.style.display = i === index ? "block" : "none";
                });
            }

            function changeSlide(direction) {
                currentSlide += direction;
                if (currentSlide < 0) currentSlide = slides.length - 1;
                if (currentSlide >= slides.length) currentSlide = 0;
                showSlide(currentSlide);
            }

            // Inicializa o primeiro slide
            showSlide(currentSlide);
            </script>

        <div id="spv" class="content-section hidden">
            <h2> Desenvolvimento do Sistema de Processamento da Vis√£o (SPV)</h2>

            <h3>Descri√ß√£o da Etapa</h3>
            <p>
                Nesta etapa, a equipe implementou o <strong>Sistema de Processamento da Vis√£o (SPV)</strong>,
                utilizando a API <code>OpenCV</code> em conjunto com as bibliotecas <code>MediaPipe</code> e <code>scikit-learn</code>. 
                O sistema foi desenvolvido em ambiente <strong>Ubuntu-Linux</strong>, com o uso da webcam do laborat√≥rio, seguindo todas as diretrizes propostas pelo trabalho: execu√ß√£o local, uso de bibliotecas reconhecidas e gratuitas, c√°lculos realizados em tempo real ("on-the-fly") e implementa√ß√£o pr√≥pria pelos integrantes da equipe.
            </p>
            <p>
                O desenvolvimento foi dividido em tr√™s c√≥digos principais, que organizam o fluxo do sistema:
                <strong>coleta de dados</strong>, <strong>treinamento do modelo</strong> e 
                <strong>predi√ß√£o em tempo real</strong>. Al√©m disso, diversos arquivos auxiliares foram gerados 
                para permitir a utiliza√ß√£o completa do SPV.
            </p>

            <h3>Ordem de Execu√ß√£o dos C√≥digos</h3>
            <ol style="padding-left: 20px;">
                <li><code>Extracao.py</code> ‚Üí Coleta de dados dos gestos via webcam e exporta√ß√£o para um arquivo CSV.</li>
                <li><code>treinarModelo.py</code> ‚Üí Treinamento do classificador a partir dos dados coletados.</li>
                <li><code>predicao.py</code> ‚Üí Execu√ß√£o do sistema em tempo real para reconhecimento de gestos.</li>
            </ol>

            <h3>Descri√ß√£o dos Arquivos de C√≥digo</h3>
            <ul style="padding-left: 20px;">
                <li>
                    <strong>Extracao.py</strong>: 
                    Utiliza a biblioteca <code>MediaPipe</code> para detectar landmarks das m√£os em tempo real. 
                    O usu√°rio pode capturar amostras de diferentes gestos, que s√£o salvas no arquivo <code>dados_gesto.csv</code>. 
                    Cada frame salvo cont√©m 63 coordenadas (x, y, z) e o r√≥tulo do gesto informado. Nos bot√µes abaixo, voc√™ pode visualizar o c√≥digo de coleta e tamb√©m um exemplo de csv gerado pela equipe nesse projeto, com 1738 frames coletados. A quantidade e a variabilidade dos frames coletados foi essencial para uma boa predi√ß√£o.
                    
                    <div style="margin-top: 10px;">
                        <a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/Extracao.py" target="_blank" style="text-decoration: none;">
                            <button style="background-color:#4CAF50; color:white; border:none; padding:10px 16px; border-radius:8px; cursor:pointer; font-size:14px; margin-right:10px;">
                                üìú Ver C√≥digo
                            </button>
                        </a>

                        <a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/dados_gesto.csv" target="_blank" style="text-decoration: none;">
                            <button style="background-color:#2196F3; color:white; border:none; padding:10px 16px; border-radius:8px; cursor:pointer; font-size:14px;">
                                üìä Ver CSV
                            </button>
                        </a>
                    </div>
                </li>

                <li>
                    <strong>treinarModelo.py</strong>: 
                    L√™ os dados do arquivo <code>dados_gesto.csv</code> gerado anteriormente e treina um classificador <code>RandomForest</code> utilizando <code>scikit-learn</code>. 
                    O modelo gerado √© salvo em <code>gesture_model.pkl</code>, 
                    enquanto os r√≥tulos codificados ficam em <code>label_encoder.pkl</code>.
                    Nos bot√µes abaixo, voc√™ pode visualizar o c√≥digo de treinamento e tamb√©m o modelo e os r√≥tulos codificados gerados pela nesse projeto.

                    <div style="margin-top: 10px;">
                        <a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/treinarModelo.py" target="_blank" style="text-decoration: none;">
                            <button style="background-color:#4CAF50; color:white; border:none; padding:10px 16px; border-radius:8px; cursor:pointer; font-size:14px; margin-right:10px;">
                                üìú Ver C√≥digo
                            </button>
                        </a>

                        <a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/gesture_model.pkl" target="_blank" style="text-decoration: none;">
                            <button style="background-color:#2196F3; color:white; border:none; padding:10px 16px; border-radius:8px; cursor:pointer; font-size:14px;">
                                üìä Ver Modelo
                            </button>
                        </a>

                         <a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/label_encoder.pkl" target="_blank" style="text-decoration: none;">
                            <button style="background-color:#2196F3; color:white; border:none; padding:10px 16px; border-radius:8px; cursor:pointer; font-size:14px;">
                                üìä Ver R√≥tulos Codificados
                            </button>
                        </a>
                    </div>
                </li>

                <li>
                    <strong>predicao.py</strong>: 
                    Carrega o modelo treinado e executa a predi√ß√£o em tempo real, utilizando a webcam. 
                    Quando um gesto √© reconhecido, o nome correspondente √© exibido diretamente sobre a imagem do v√≠deo.
                    No bot√£o abaixo, voc√™ pode visualizar o c√≥digo de predi√ß√£o.

                    <div style="margin-top: 10px;">
                        <a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/predicao.py" target="_blank" style="text-decoration: none;">
                                <button style="background-color:#4CAF50; color:white; border:none; padding:10px 16px; border-radius:8px; cursor:pointer; font-size:14px; margin-right:10px;">
                                    üìú Ver C√≥digo
                                </button>
                        </a>
                    </div>
                </li>

            </ul>

            <h3>Arquivos Gerados e Utilizados</h3>
            <ul style="padding-left: 20px;">
                <li><strong>dados_gesto.csv</strong>: Base de dados coletada com os landmarks das m√£os e os r√≥tulos dos gestos.</li>
                <li><strong>gesture_model.pkl</strong>: Modelo de classifica√ß√£o treinado para reconhecimento dos gestos.</li>
                <li><strong>label_encoder.pkl</strong>: Codificador dos r√≥tulos, permitindo a tradu√ß√£o de classes num√©ricas para nomes de gestos.</li>
                <li><strong>libs.txt</strong>: Arquivo listando as bibliotecas utilizadas no desenvolvimento do sistema (como depend√™ncias).</li>
            </ul>

            <h3>Considera√ß√µes</h3>
            <p>
                O sistema desenvolvido atende √†s necessidades previstas pelo Contexto e Cen√°rio de Aplica√ß√£o (CA), 
                permitindo que <strong>usu√°rios leigos</strong> possam execut√°-lo de forma simples e intuitiva, 
                visualizando os resultados em tempo real. 
                Al√©m disso, todos os c√≥digos incluem cabe√ßalhos documentados com nome dos autores, RA, data, 
                nome do programa e instru√ß√µes de execu√ß√£o no Linux, conforme solicitado.
            </p>
        </div>

        <div id="laboratorio-experimental" class="content-section hidden">
            <h2>Laborat√≥rio Experimental do SPV (LEx)</h2>

            <h3>Introdu√ß√£o</h3>
            <p>
                Este roteiro tem como objetivo principal orientar qualquer usu√°rio, mesmo sem conhecimento t√©cnico pr√©vio, a utilizar o Sistema Tradutor de Libras para Texto desenvolvido pela equipe, seja no laborat√≥rio de aula ou em qualquer outro ambiente com condi√ß√µes semelhantes, e assim obter seus pr√≥prios resultados experimentais.
            </p>
            <p>
                Relembrando brevemente, o sistema desenvolvido √© um prot√≥tipo que utiliza t√©cnicas de Vis√£o Computacional e Processamento de V√≠deo para traduzir sinais espec√≠ficos da L√≠ngua Brasileira de Sinais (Libras) para texto escrito em portugu√™s. Seu foco √© a aplica√ß√£o em contextos cl√≠nicos e hospitalares, com capacidade de reconhecer sinais usados em triagens de pronto-socorro ou consultas cl√≠nicas b√°sicas, como sintomas, dores e respostas simples.
            </p>
            <p>
                O sistema opera por meio de m√≥dulos que atuam de forma sequencial: captura de v√≠deo em tempo real de uma pessoa sinalizando, pr√©-processamento dos frames, extra√ß√£o de caracter√≠sticas das m√£os, classifica√ß√£o dos sinais e, por fim, exibi√ß√£o do texto traduzido na tela.
            </p>
            <p>
                Nesta vers√£o do projeto, a equipe projetou o sistema para identificar os seguintes sinais: <strong>N√£o</strong>, <strong>Sim</strong>, <strong>Dor</strong>, <strong>Febre</strong>, <strong>Enjoo</strong> e <strong>Resfriado</strong>.
            </p>
            <p>
                Para este laborat√≥rio experimental, o usu√°rio n√£o precisar√° realizar etapas de coleta de dados ou treinamento do modelo, todo o sistema j√° est√° configurado e treinado. O objetivo ser√° apenas executar o m√≥dulo de predi√ß√£o em tempo real, observar o reconhecimento dos gestos e registrar os resultados obtidos. O v√≠deo abaixo demonstra como o usu√°rio pode utilizar o sistema atrav√©s de suas interfaces de entrada e sa√≠da.
            </p>

            <video controls style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
                <source src="etapa5-Lex/video.mp4" type="video/mp4">
                Seu navegador n√£o suporta a exibi√ß√£o de v√≠deos.
            </video>

            <h3>Procedimento Experimental</h3>
            <p><strong>Pr√©-requisitos:</strong></p>
            <ul style="padding-left: 20px;">
                <li>Computador com Linux</li>
                <li>Python 3.10 (devido a compatibilidade com algumas libs utilizadas)</li>
                <li>Bibliotecas necess√°rias: <code>opencv-python</code>, <code>mediapipe</code>, <code>scikit-learn</code>, <code>numpy</code>, <code>pandas</code></li>
                <li>Modelo treinado (<code>gesture_model.pkl</code>) e codificador (<code>label_encoder.pkl</code>) j√° dispon√≠veis no reposit√≥rio da equipe</li>
                <li>C√≥digo para predi√ß√£o dos sinais <code>predicao.py</code>, tamb√©m dispon√≠vel no reposit√≥rio da equipe.</li>
            </ul>

            <h4>Execu√ß√£o do Sistema</h4>
            <ol style="padding-left: 20px;">
                <li>Realize o download dos arquivos do projeto, dispon√≠vel <a href="https://github.com/LeonardoMaioli/computer_vision_2025/tree/main/etapa4-SPV" target="_blank">neste link</a> ou clicando no bot√£o da etapa 4 dessa p√°gina, onde est√£o detalhados cada arquivo disponibilizado.</li>
                <li>Em sua m√°quina, crie um ambiente virtual com o python necess√°rio, utilizando o comando <code>conda create -n cv25_py310 python=3.10</code></li>
                <li>Nesse ambiente virtual criado, instale as bibliotecas necess√°rias e que n√£o s√£o nativas do Python com os comandos: <code>pip install opencv-python</code>,<code>pip install mediapipe</code> e <code>pip install scikit-learn</code></li>
                <li>Abra o terminal e navegue at√© a pasta onde salvou o diret√≥rio do projeto.</li>
                <li>Ative o ambiente virtual criado com o comando: <code>conda activate cv25_py310</code></li>
                <li>Conecte a webcam no computador e verifique que a mesma esteja conectada e funcionando.</li>
                <li>De volta ao terminal, execute o comando: <code>python3 predicao.py</code></li>
                <li>O sistema abrir√° a webcam que ficar√° gravando em tempo real e exibir√°, no terminal e na tela, o nome do gesto detectado realizado pela pessoa.</li>
                <li>Realize os gestos um de cada vez, mantendo a m√£o vis√≠vel para a c√¢mera e posicionada de forma centralizada no quadro.</li>
                <li>Para encerrar a execu√ß√£o, pressione <strong>"q"</strong> no teclado.</li>
            </ol>

            <h4>Sugest√£o de Execu√ß√µes para o Teste</h4>
            <ul style="padding-left: 20px;">
                <li>Realizar cada um dos gestos dispon√≠veis: <strong>N√£o</strong>, <strong>Sim</strong>, <strong>Dor</strong>, <strong>Febre</strong>, <strong>Enjoo</strong> e <strong>Resfriado</strong>.</li>
                <li>Se o usu√°rio n√£o souber como fazer um sinal, recomendamos utilizar o aplicativo <strong>HandTalk</strong>: basta escrever o nome do sinal (ex.: ‚Äúdor‚Äù) e o avatar exibir√° como realizar o movimento. Caso n√£o tenha acesso ao aplicativo, pode pedir ajuda a algum dos membros do grupo.</li>
                <li>Variar a dist√¢ncia da c√¢mera (perto e longe) para avaliar se o reconhecimento se mant√©m.</li>
                <li>Alterar levemente o √¢ngulo da m√£o para verificar a robustez do reconhecimento.</li>
                <li>Realizar gestos em sequ√™ncia r√°pida para observar o tempo de resposta do sistema.</li>
                <li>Repetir alguns gestos para verificar consist√™ncia na detec√ß√£o.</li>
            </ul>

            <h4>Resultados Esperados</h4>
            <p>O usu√°rio deve ver o nome do gesto reconhecido pelo sistema. Caso n√£o seja identificado, nada ser√° exibido.</p>

            <h4>Registro de Resultados</h4>
            <p>
                Durante a execu√ß√£o, o usu√°rio deve anotar:
            </p>
            <ul style="padding-left: 20px;">
                <li>Quais gestos foram reconhecidos corretamente.</li>
                <li>Se houve erros de classifica√ß√£o.</li>
                <li>Observa√ß√µes sobre tempo de resposta e facilidade de uso.</li>
            </ul>

            <h3>Question√°rio</h3>
            <p>Para avaliar os resultados obtidos pelos usu√°rios e seu entendimento sobre o sistema, a equipe criou um question√°rio:</p>
            <a href="https://docs.google.com/forms/d/e/1FAIpQLSc03dsRAjM-DW0ZTd4RoBzOcKJShRaTjKwCCOGy1Y-WUTH5SQ/viewform?usp=header" 
                target="_blank" 
                style="display: inline-block; padding: 12px 24px; background: linear-gradient(135deg, #4CAF50, #45a049); color: #fff; text-decoration: none; border-radius: 10px; font-size: 16px; font-weight: bold; transition: 0.3s;">
                üìë Abrir Question√°rio
            </a>
            <p>As perguntas utilizadas no question√°rio foram:</p>
            <ol style="padding-left: 20px;">
                <li>O sistema conseguiu reconhecer corretamente todos os gestos realizados (<strong>N√£o</strong>, <strong>Sim</strong>, <strong>Dor</strong>, <strong>Febre</strong>, <strong>Enjoo</strong>, <strong>Resfriado</strong>)? Se n√£o, qual teve mais dificuldade?</li>
                <li>O reconhecimento se manteve preciso mesmo variando a dist√¢ncia da c√¢mera? </li>
                <li>O reconhecimento se manteve preciso mesmo alterando levemente o √¢ngulo da m√£o? </li>
                <li>O sistema respondeu rapidamente aos gestos realizados?</li>
                <li>Houve consist√™ncia no reconhecimento quando voc√™ repetiu os mesmos gestos?</li>
                <li>As instru√ß√µes para realizar os gestos estavam claras e ajudaram na execu√ß√£o? </li>
                <li>Foi f√°cil pedir ajuda ou usar o aplicativo <strong>HandTalk</strong> para aprender gestos desconhecidos? </li>
                <li>O sistema foi f√°cil de usar de forma geral? </li>
            </ol>

            <h3>Enquete Subjetiva</h3>
            <p>Al√©m do question√°rio, o grupo tamb√©m disponibilizou uma enquete ao usu√°rio com perguntas sobre a sua opini√£o em rela√ß√£o ao sistema:</p> 
            <a href="https://docs.google.com/forms/d/e/1FAIpQLScinVG_XWMuiE9JYqSwbNiGoocwTjUwt-QvRPGYjMkmibgx4A/viewform?usp=header" 
                target="_blank" 
                style="display: inline-block; padding: 12px 24px; background: linear-gradient(135deg, #4CAF50, #45a049); color: #fff; text-decoration: none; border-radius: 10px; font-size: 16px; font-weight: bold; transition: 0.3s;">
                üìë Abrir Enquete
            </a>
            
            <p>Essa enquete possui dois tipos de perguntas: (a) com respostas escritas; e (b) com respostas em escala 1~5. As perguntas utilizadas foram:</p>
            <p><strong>Perguntas abertas:</strong></p>
            <ul style="padding-left: 20px;">
                <li>O que mais gostou no sistema?</li>
                <li>O que acha que poderia melhorar?</li>
                <li>Em quais contextos reais voc√™ imagina o uso deste sistema?</li>
            </ul>
            <p><strong>Perguntas de escala (1 a 5):</strong></p>
            <ul style="padding-left: 20px;">
                <li>Facilidade de uso</li>
                <li>Clareza das instru√ß√µes</li>
                <li>Qualidade do reconhecimento</li>
                <li>Aplicabilidade pr√°tica</li>
            </ul>
        </div>

        <div id="teste-campo" class="content-section hidden">
            <h2>Teste de Campo do SPV (TC)</h2>

            <h3>Descri√ß√£o do Teste de Campo</h3>
            <p>
                O <strong>Teste de Campo</strong> consistiu na aplica√ß√£o pr√°tica do Sistema Tradutor de Libras para Texto em um ambiente de laborat√≥rio com os demais alunos da turma. 
                Essa etapa teve como objetivo avaliar a usabilidade e a precis√£o do sistema em um cen√°rio mais pr√≥ximo de sua aplica√ß√£o real, envolvendo participantes de diferentes perfis, membros de outras equipes da disciplina.
            </p>
            <p>
                O procedimento seguiu o <em>Roteiro do Laborat√≥rio Experimental</em> da etapa anterior, no qual cada usu√°rio interagiu com o sistema realizando gestos espec√≠ficos, e em seguida, respondeu ao question√°rio estruturado via <strong>Google Forms</strong>. 
                A coleta das respostas permitir√° uma an√°lise dos acertos do sistema, do tempo de resposta e da facilidade de uso, al√©m de observa√ß√µes subjetivas sobre a experi√™ncia. 
            </p>
            <p>
                Durante o teste, foi poss√≠vel observar que o sistema se mostrou intuitivo e de f√°cil utiliza√ß√£o, mesmo para pessoas sem conhecimento pr√©vio em Libras ou em tecnologias de vis√£o computacional. 
                Tamb√©m foi realizado o registro audiovisual de usu√°rios utilizando o sistema, mediante consentimento, com o intuito de documentar a execu√ß√£o pr√°tica desta etapa.
            </p>

            <h3>V√≠deos do Teste de Campo</h3>
            <p>A seguir, apresentamos dois registros em v√≠deo de usu√°rios utilizando o sistema no laborat√≥rio:</p>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                <video controls style="width: 100%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);">
                    <source src="etapa6-TC/video1.mp4" type="video/mp4">
                    Seu navegador n√£o suporta a exibi√ß√£o de v√≠deos.
                </video>

                <video controls style="width: 100%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);">
                    <source src="etapa6-TC/video2.mp4" type="video/mp4">
                    Seu navegador n√£o suporta a exibi√ß√£o de v√≠deos.
                </video>
            </div>
        </div>

        <div id="relatorio-final" class="content-section hidden">
            <h2>Relat√≥rio Final do Trabalho (RFT)</h2>

            <p style="margin-top: -8px; color:#555;">Resumo consolidado do projeto ‚ÄúSistema tradutor de Libras para texto em portugu√™s ‚Äî Aplica√ß√£o em contextos cl√≠nicos e hospitalares‚Äù.</p>

            <h3>1. Introdu√ß√£o</h3>
            <p>
                O objetivo geral deste trabalho foi desenvolver um <strong>prot√≥tipo funcional</strong> capaz de <strong>traduzir sinais de Libras para texto em portugu√™s</strong> em tempo real,
                com foco de uso em <strong>cl√≠nicas e hospitais</strong>. Os objetivos espec√≠ficos foram: (i) projetar o fluxo do sistema (captura, pr√©-processamento,
                extra√ß√£o de caracter√≠sticas, classifica√ß√£o e exibi√ß√£o do texto); (ii) <strong>implementar</strong> um SPV utilizando bibliotecas livres;
                (iii) <strong>coletar dados</strong> reais e treinar um classificador; (iv) <strong>avaliar</strong> desempenho, robustez e usabilidade em laborat√≥rio e em teste de campo;
                (v) <strong>documentar</strong> todo o processo para reprodutibilidade.
            </p>

            <h4>1.2. Cen√°rio de Aplica√ß√£o (CA)</h4>
            <p>
                Todo o contexto, motiva√ß√£o e justificativas est√£o descritos na <a href="#" onclick="showContent('cenario-aplicacao')">Etapa 1 ‚Äî CA</a>.
                Em s√≠ntese, a equipe observou por meio de entrevistas e pesquisas a exist√™ncia de uma <strong>lacuna</strong> de solu√ß√µes acess√≠veis para tradu√ß√£o autom√°tica na dire√ß√£o <em>Libras ‚Üí Portugu√™s</em> em ambientes reais,
                o que impacta diretamente a comunica√ß√£o entre <strong>profissionais da sa√∫de</strong> e <strong>pacientes surdos</strong>.
            </p>

            <h4>1.3. Fundamenta√ß√£o Te√≥rica</h4>
            <p>
                Do ponto de vista t√©cnico, este trabalho se apoia em conceitos de vis√£o computacional e aprendizado de
                m√°quina. A tradu√ß√£o de Libras para texto foi baseada na detec√ß√£o de <em>landmarks</em> das m√£os, que
                permitem representar a postura e o movimento dos gestos. Ap√≥s a captura, os dados passam por um
                processo de pr√©-processamento que envolve a organiza√ß√£o dos <em>landmarks</em> em vetores num√©ricos,
                normaliza√ß√£o e limpeza de amostras ruidosas. Para o reconhecimento dos sinais, foi treinado um modelo
                de <em>Random Forest</em> a partir de amostras rotuladas de gestos, o que garante bom desempenho em
                termos de acur√°cia e rapidez de predi√ß√£o. Por fim, todo o sistema foi estruturado em um pipeline em
                tempo real, capaz de realizar captura com webcam, extra√ß√£o de caracter√≠sticas e predi√ß√£o cont√≠nua
                ‚Äúon-the-fly‚Äù.
            </p>

            <h3>2. Materiais e M√©todos</h3>
            <h4>2.1 Modelagem Funcional do SPV (MF)</h4>
            <p>
                O diagrama de blocos e a descri√ß√£o dos m√≥dulos encontram‚Äëse em <a href="#" onclick="showContent('modelagem-funcional')">Etapa 2 ‚Äî MF</a>.
            </p>
            <p>
                Em s√≠ntese, o sistema foi concebido como um pipeline sequencial que parte da captura de v√≠deo em tempo real, realiza o pr√©-processamento dos frames e a detec√ß√£o das regi√µes de interesse (m√£os). 
                Em seguida, procede-se √† extra√ß√£o de caracter√≠sticas relevantes (landmarks), que s√£o organizadas em vetores num√©ricos e fornecidas a um classificador de aprendizado de m√°quina respons√°vel por identificar os sinais de Libras. 
                O resultado √© convertido em texto em portugu√™s e exibido em interface gr√°fica. 
                Essa modelagem funcional permitiu estruturar de forma clara o fluxo de dados, bem como os requisitos t√©cnicos para cada etapa do SPV, desde a entrada at√© a sa√≠da final do sistema.
            </p>

            <h4>2.2 Descri√ß√£o da Implementa√ß√£o do SPV</h4>
            <p>
                Implementa√ß√£o em <strong>Python 3.10</strong> com <code>OpenCV</code>, <code>MediaPipe</code> e <code>scikit-learn</code>, seguindo os requisitos de execu√ß√£o local
                e bibliotecas livres. O desenvolvimento foi organizado em tr√™s scripts principais:
            </p>
            <ol style="padding-left: 20px;">
                <li><code>Extracao.py</code> ‚Äî captura landmarks da m√£o e salva amostras rotuladas em <code>dados_gesto.csv</code>.</li>
                <li><code>treinarModelo.py</code> ‚Äî treina o classificador <em>Random Forest</em> e salva <code>gesture_model.pkl</code> e <code>label_encoder.pkl</code>.</li>
                <li><code>predicao.py</code> ‚Äî realiza a predi√ß√£o em tempo real via webcam, exibindo o gesto reconhecido.</li>
            </ol>
            <p>
                Uma descri√ß√£o mais detalhada da implementa√ß√£o do SPV encontram‚Äëse em <a href="#" onclick="showContent('spv')">Etapa 4 ‚Äî SPV</a>.
            </p>

            <h4>2.3 Lista de Arquivos (c√≥digo‚Äëfonte, dados e auxiliares)</h4>
            <ul style="padding-left: 20px;">
                <li><a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/Extracao.py" target="_blank">Extracao.py</a></li>
                <li><a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/treinarModelo.py" target="_blank">treinarModelo.py</a></li>
                <li><a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/predicao.py" target="_blank">predicao.py</a></li>
                <li><a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/dados_gesto.csv" target="_blank">dados_gesto.csv</a> (amostras coletadas)</li>
                <li><a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/gesture_model.pkl" target="_blank">gesture_model.pkl</a> (modelo treinado)</li>
                <li><a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/label_encoder.pkl" target="_blank">label_encoder.pkl</a> (r√≥tulos codificados)</li>
                <li><a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto/libs.txt" target="_blank">libs.txt</a> (lista de depend√™ncias)</li>
                <li>V√≠deos do SPV em funcionamento tamb√©m s√£o encontrados em <a href="#" onclick="showContent('spv')">Etapa 4 ‚Äî SPV</a>.</li>
            </ul>

            <h4>2.4 An√°lise T√©cnica ‚Äî M√©tricas e Atendimento ao Cen√°rio</h4>
            <p>
                Para avaliar o grau de atendimento ao cen√°rio escolhido, foram definidas m√©tricas baseadas em um 
                question√°rio aplicado aos participantes durante os testes. Essas m√©tricas buscavam captar a percep√ß√£o dos 
                usu√°rios sobre diferentes aspectos do sistema, funcionando como indicadores qualitativos iniciais:
            </p>
            <ul style="padding-left:20px;">
                <li>Precis√£o do reconhecimento dos gestos realizados;</li>
                <li>Robustez do reconhecimento frente a varia√ß√µes de dist√¢ncia da c√¢mera;</li>
                <li>Robustez do reconhecimento frente a pequenas altera√ß√µes no √¢ngulo da m√£o;</li>
                <li>Rapidez de resposta do sistema aos gestos;</li>
                <li>Consist√™ncia no reconhecimento de gestos repetidos;</li>
                <li>Clareza e utilidade das instru√ß√µes fornecidas para a realiza√ß√£o dos gestos;</li>
                <li>Facilidade de utiliza√ß√£o de recursos de apoio para aprendizado de gestos desconhecidos;</li>
                <li>Usabilidade geral do sistema.</li>
            </ul>
            <p>
                Essas m√©tricas atuaram como nossas <strong>m√©tricas qualitativas iniciais</strong>, permitindo identificar pontos fortes 
                (como a rapidez de resposta e a clareza das instru√ß√µes) e limita√ß√µes (como varia√ß√µes de precis√£o em diferentes √¢ngulos ou dist√¢ncias). 
                Embora ainda n√£o tenham sido acompanhadas de valores num√©ricos objetivos (como taxas de acerto por gesto, tempo m√©dio de resposta ou 
                varia√ß√£o estat√≠stica), esse levantamento subjetivo serviu como uma primeira etapa de valida√ß√£o pr√°tica do prot√≥tipo.
            </p>
            <p>
                Como <strong>continuidade natural</strong> deste trabalho, prop√µe-se a evolu√ß√£o dessas m√©tricas para indicadores quantitativos, de modo a obter 
                resultados objetivos e compar√°veis em futuras vers√µes do sistema. Por exemplo: calcular a acur√°cia m√©dia por gesto, medir a lat√™ncia em milissegundos, 
                avaliar a taxa de repetibilidade em diferentes condi√ß√µes de ilumina√ß√£o ou dist√¢ncia, entre outras an√°lises que possam consolidar ainda mais 
                a robustez do sistema.
            </p>
            <p style="margin-top:10px;">
                <strong>Conclus√£o da an√°lise t√©cnica:</strong> O prot√≥tipo desenvolvido atende de forma <strong>parcial</strong> o
                cen√°rio de aplica√ß√£o proposto. Essa limita√ß√£o ocorre porque, para viabilizar a implementa√ß√£o dentro do
                escopo do projeto, foram selecionados apenas alguns sinais espec√≠ficos de Libras para a predi√ß√£o
                (<em>N√£o</em>, <em>Sim</em>, <em>Dor</em>, <em>Febre</em>, <em>Enjoo</em> e <em>Resfriado</em>), em vez de um
                vocabul√°rio amplo e variado, como seria necess√°rio para atender plenamente √†s demandas reais de
                comunica√ß√£o em ambientes cl√≠nicos e hospitalares.
            </p>
            <p>
                No entanto, os resultados alcan√ßados demonstraram que, mesmo com esse vocabul√°rio reduzido, o sistema
                foi capaz de reconhecer os sinais escolhidos com boa acur√°cia, baixa lat√™ncia e comportamento consistente.
                Isso evidencia que o prot√≥tipo cumpre com √™xito o papel de <strong>prova de conceito</strong>, validando a
                viabilidade t√©cnica da abordagem proposta e mostrando que √© poss√≠vel traduzir sinais de Libras para texto
                em tempo real com tecnologias acess√≠veis de vis√£o computacional e aprendizado de m√°quina.
            </p>
            <p>
                Portanto, o trabalho abre caminho para <strong>projetos futuros</strong> que possam expandir o vocabul√°rio de
                sinais reconhecidos, melhorar a robustez frente a diferentes condi√ß√µes de ilumina√ß√£o e √¢ngulo, e aplicar
                modelos mais sofisticados de aprendizado profundo. Dessa forma, ainda que o alcance atual seja limitado,
                o sistema representa um passo inicial relevante em dire√ß√£o a uma solu√ß√£o pr√°tica para inclus√£o comunicacional
                em contextos de sa√∫de e em outros cen√°rios de intera√ß√£o entre surdos e ouvintes.
            </p>

            <h3>3. Laborat√≥rio Experimental</h3>
            <h4>3.1 Roteiro do Laborat√≥rio</h4>
            <p>
                O roteiro passo a passo de execu√ß√£o est√° documentado na <a href="#" onclick="showContent('laboratorio-experimental')">Etapa 5 ‚Äî LEx</a>, incluindo prepara√ß√£o do ambiente, comandos, procedimentos e registros.
            </p>

            <h4>3.2 Teste de Campo (TCS) ‚Äî An√°lise de Resultados</h4>
            <p>
                Os registros em v√≠deo do teste de campo est√£o na <a href="#" onclick="showContent('teste-campo')">Etapa 6 ‚Äî TC</a>.
            </p>
            <p>
                Durante o Teste de Campo, foram conduzidos experimentos estruturados para avaliar a precis√£o, robustez e usabilidade do Sistema Tradutor de Libras para Texto (SPV) em condi√ß√µes controladas de laborat√≥rio. Os experimentos inclu√≠ram:
            </p>
            <ul style="padding-left: 20px;">
                <li>Execu√ß√£o de cada um dos sinais programados no sistema: <strong>N√£o</strong>, <strong>Sim</strong>, <strong>Dor</strong>, <strong>Febre</strong>, <strong>Enjoo</strong> e <strong>Resfriado</strong>;</li>
                <li>Varia√ß√£o da dist√¢ncia entre a m√£o e a c√¢mera para avaliar a robustez do reconhecimento;</li>
                <li>Altera√ß√£o do √¢ngulo da m√£o durante a execu√ß√£o do gesto para testar consist√™ncia do modelo;</li>
                <li>Execu√ß√£o de gestos em sequ√™ncia r√°pida para verificar o tempo de resposta do sistema;</li>
                <li>Repeti√ß√£o de gestos em diferentes momentos para avaliar a consist√™ncia na detec√ß√£o;</li>
                <li>Utiliza√ß√£o de aplicativos de apoio (como HandTalk) quando o usu√°rio n√£o sabia executar determinado gesto;</li>
                <li>Registro de observa√ß√µes sobre facilidade de uso, clareza das instru√ß√µes e experi√™ncia geral durante a intera√ß√£o com o sistema.</li>
            </ul>

            <h3>Crit√©rios de Avalia√ß√£o</h3>
            <p>
                Os experimentos foram avaliados a partir de m√©tricas qualitativas e subjetivas, coletadas via question√°rio e enquete, conforme descrito no laborat√≥rio experimental:
            </p>
            <ul style="padding-left: 20px;">
                <li><strong>Precis√£o do Reconhecimento:</strong> capacidade do sistema em identificar corretamente os sinais realizados pelos usu√°rios;</li>
                <li><strong>Robustez:</strong> manuten√ß√£o da precis√£o frente a varia√ß√µes de dist√¢ncia e √¢ngulo da m√£o;</li>
                <li><strong>Tempo de Resposta:</strong> rapidez do sistema em exibir o sinal reconhecido ap√≥s o gesto ser realizado;</li>
                <li><strong>Consist√™ncia:</strong> estabilidade do reconhecimento ao repetir os mesmos gestos;</li>
                <li><strong>Usabilidade:</strong> facilidade de execu√ß√£o e compreens√£o das instru√ß√µes pelos usu√°rios;</li>
                <li><strong>Aplicabilidade Pr√°tica:</strong> percep√ß√£o dos usu√°rios sobre o uso do sistema em contextos cl√≠nicos ou reais.</li>
            </ul>

            <h3>An√°lise das M√©dias e Observa√ß√µes Subjetivas</h3>
            <p>
                A partir do question√°rio e da enquete aplicada, foi poss√≠vel consolidar uma vis√£o geral do desempenho do sistema:
            </p>
            <ul style="padding-left: 20px;">
                <li>Todos os sinais foram reconhecidos corretamente, demonstrando precis√£o adequada para os sinais testados;</li>
                <li>Alguns gestos apresentaram pequenas varia√ß√µes de acerto quando a dist√¢ncia ou o √¢ngulo foram alterados, indicando pontos de melhoria no pr√©-processamento e treinamento do modelo de predi√ß√£o;</li>
                <li>O tempo de resposta do sistema foi considerado r√°pido, garantindo uma intera√ß√£o fluida com o usu√°rio;</li>
                <li>As instru√ß√µes foram consideradas claras e facilitaram a execu√ß√£o correta dos gestos;</li>
                <li>Os usu√°rios avaliaram a usabilidade do sistema como satisfat√≥ria, apontando potencial aplica√ß√£o em contextos cl√≠nicos de triagem e atendimento r√°pido.</li>
            </ul>

            <p>
                Essa an√°lise inicial permite identificar os pontos fortes do prot√≥tipo, como rapidez e clareza na intera√ß√£o, bem como limita√ß√µes, principalmente em termos de robustez frente a varia√ß√µes de √¢ngulo e dist√¢ncia. Como pr√≥ximos passos, recomenda-se a implementa√ß√£o de m√©tricas quantitativas, como acur√°cia por gesto, lat√™ncia em milissegundos e taxa de repetibilidade, de modo a tornar os resultados compar√°veis e objetivamente mensur√°veis em futuras vers√µes do sistema.
            </p>

            <h3>4. Conclus√µes</h3>
            <p>
                Os objetivos propostos foram <strong>majoritariamente atingidos</strong>:
                o prot√≥tipo demonstrou viabilidade t√©cnica para tradu√ß√£o b√°sica de Libras ‚Üí texto em tempo real.
                <strong>Pontos positivos:</strong> execu√ß√£o local, bibliotecas livres, facilidade de uso e resposta r√°pida.
                <strong>Pontos a melhorar:</strong> ampliar vocabul√°rio, aumentar robustez a condi√ß√µes adversas,
                coletar base de dados maior/diversa e explorar modelos mais avan√ßados.
            </p>

            <h3>5. Refer√™ncias Bibliogr√°ficas</h3>
            <p>
                As refer√™ncias utilizadas est√£o consolidadas na se√ß√£o <a href="#" onclick="showContent('referencias')">Refer√™ncias</a> desta p√°gina.
            </p>

            <h3>6. Anexos</h3>
            <p>
                O conjunto completo de <strong>c√≥digos, dados e m√≠dias</strong> est√° listado na subse√ß√£o <em>2.3</em> acima e nas Etapas 4‚Äì6.
                O acesso a todos esses arquivos para download tamb√©m pode ser feito por aqui, clicando no bot√£o abaixo.
            </p>
            <div style="margin-top: 10px;">
                <a href="https://github.com/LeonardoMaioli/computer_vision_2025/blob/main/etapa4-SPV/Arquivos_Projeto" target="_blank" style="text-decoration: none;">
                    <button style="background-color:#4CAF50; color:white; border:none; padding:10px 16px; border-radius:8px; cursor:pointer; font-size:14px; margin-right:10px;">
                        üìú Ir para reposit√≥rio de arquivos
                    </button>
                </a>
            </div>

        </div>

        <div id="referencias" class="content-section hidden">
            <h2>Refer√™ncias</h2>
            <ul style="list-style-type: decimal; padding-left: 20px;">
                <li>
                    HAND TALK. <em>Hand Talk Tradutor</em>. Dispon√≠vel em: 
                    <a href="https://www.handtalk.me/br/" target="_blank">https://www.handtalk.me/br/</a>. 
                    Acesso em: 17 jun. 2025.
                </li>
                <li>
                    VLIBRAS. <em>Plataforma VLibras</em>. Dispon√≠vel em: 
                    <a href="https://www.vlibras.gov.br/" target="_blank">https://www.vlibras.gov.br/</a>. 
                    Acesso em: 17 jun. 2025.
                </li>
                <li>
                    PAPATSIMOULI, Maria; SARIGIANNIDIS, Panos; FRAGULIS, George F. 
                    <em>A survey of advancements in real-time sign language translators: Integration with IoT technology</em>. 
                    <strong>Technologies</strong>, v. 11, n. 4, p. 83, 2023. DOI: 
                    <a href="https://doi.org/10.3390/technologies11040083" target="_blank">10.3390/technologies11040083</a>.
                </li>
                <li>
                    NATH, Geethu G.; ARUN, C. S. <em>Real time sign language interpreter</em>. 
                    In: <strong>IEEE International Conference on Electrical, Instrumentation and Communication Engineering (ICEICE)</strong>, 
                    2017, Karur, √çndia. Anais [...]. IEEE, 2017. DOI: 
                    <a href="https://doi.org/10.1109/ICEICE.2017.8191869" target="_blank">10.1109/ICEICE.2017.8191869</a>.
                </li>
            </ul>
        </div>
    </div>
</div>

  <script>
    function showContent(id) {
      const sections = document.querySelectorAll('.content-section');
      sections.forEach(section => {
        section.classList.add('hidden');
      });
      document.getElementById(id).classList.remove('hidden');
    }
  </script>

</body>
</html>
